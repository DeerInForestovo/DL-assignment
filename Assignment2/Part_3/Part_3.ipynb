{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Part 3\n",
    "\n",
    "## Task 1\n",
    "\n",
    "See ```train.py``` and ```vanilla_rnn.py```. I have also completed the missing code (implementation of function ```accuracy```) in ```utils.py```.\n",
    "\n",
    "## Task 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d12456c0d144b0f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current length: 3\n",
      "device: cuda\n",
      "Warning: total_len is larger than the maximum possible length. \n",
      "Setting total_len to the maximum possible length. \n",
      "Warning: access length of dataset by len(dataset) to get the actual length. \n",
      "epoch 0, training loss 2.3063433170318604, training accuracy 10.0\n",
      "epoch 0, evaluating loss 2.3572304248809814, evaluating accuracy 0.0\n",
      "epoch 1, training loss 2.28790283203125, training accuracy 13.75\n",
      "epoch 1, evaluating loss 2.3604936599731445, evaluating accuracy 5.0\n",
      "epoch 2, training loss 2.2707386016845703, training accuracy 20.0\n",
      "epoch 2, evaluating loss 2.344120502471924, evaluating accuracy 10.0\n",
      "epoch 3, training loss 2.2379698753356934, training accuracy 25.0\n",
      "epoch 3, evaluating loss 2.3290698528289795, evaluating accuracy 10.0\n",
      "epoch 4, training loss 2.209141254425049, training accuracy 23.75\n",
      "epoch 4, evaluating loss 2.311021327972412, evaluating accuracy 10.0\n",
      "epoch 5, training loss 2.1996939182281494, training accuracy 23.75\n",
      "epoch 5, evaluating loss 2.3439793586730957, evaluating accuracy 5.0\n",
      "epoch 6, training loss 2.2031524181365967, training accuracy 23.75\n",
      "epoch 6, evaluating loss 2.304316520690918, evaluating accuracy 10.0\n",
      "epoch 7, training loss 2.180318832397461, training accuracy 26.25\n",
      "epoch 7, evaluating loss 2.298139810562134, evaluating accuracy 10.0\n",
      "epoch 8, training loss 2.160701036453247, training accuracy 30.0\n",
      "epoch 8, evaluating loss 2.2928812503814697, evaluating accuracy 10.0\n",
      "epoch 9, training loss 2.1392595767974854, training accuracy 33.75\n",
      "epoch 9, evaluating loss 2.29020094871521, evaluating accuracy 10.0\n",
      "epoch 10, training loss 2.1207351684570312, training accuracy 35.0\n",
      "epoch 10, evaluating loss 2.2825703620910645, evaluating accuracy 10.0\n",
      "epoch 11, training loss 2.120265245437622, training accuracy 36.25\n",
      "epoch 11, evaluating loss 2.3259785175323486, evaluating accuracy 5.0\n",
      "epoch 12, training loss 2.177354097366333, training accuracy 23.75\n",
      "epoch 12, evaluating loss 2.264425039291382, evaluating accuracy 15.0\n",
      "epoch 13, training loss 2.1052680015563965, training accuracy 36.25\n",
      "epoch 13, evaluating loss 2.2769222259521484, evaluating accuracy 10.0\n",
      "epoch 14, training loss 2.0922389030456543, training accuracy 37.5\n",
      "epoch 14, evaluating loss 2.251110076904297, evaluating accuracy 15.0\n",
      "epoch 15, training loss 2.08594012260437, training accuracy 40.0\n",
      "epoch 15, evaluating loss 2.255312204360962, evaluating accuracy 15.0\n",
      "epoch 16, training loss 2.0735278129577637, training accuracy 45.0\n",
      "epoch 16, evaluating loss 2.22965931892395, evaluating accuracy 20.0\n",
      "epoch 17, training loss 2.0670981407165527, training accuracy 40.0\n",
      "epoch 17, evaluating loss 2.234675884246826, evaluating accuracy 20.0\n",
      "epoch 18, training loss 2.0576610565185547, training accuracy 47.5\n",
      "epoch 18, evaluating loss 2.2148823738098145, evaluating accuracy 20.0\n",
      "epoch 19, training loss 2.0501370429992676, training accuracy 40.0\n",
      "epoch 19, evaluating loss 2.2125325202941895, evaluating accuracy 20.0\n",
      "Done training.\n",
      "Current length: 4\n",
      "device: cuda\n",
      "Warning: total_len is larger than the maximum possible length. \n",
      "Setting total_len to the maximum possible length. \n",
      "Warning: access length of dataset by len(dataset) to get the actual length. \n",
      "epoch 0, training loss 2.3154412364959716, training accuracy 8.5\n",
      "epoch 0, evaluating loss 2.316997947692871, evaluating accuracy 9.5\n",
      "epoch 1, training loss 2.2851855850219724, training accuracy 14.75\n",
      "epoch 1, evaluating loss 2.2703609371185305, evaluating accuracy 15.0\n",
      "epoch 2, training loss 2.219247283935547, training accuracy 22.125\n",
      "epoch 2, evaluating loss 2.242445468902588, evaluating accuracy 19.5\n",
      "epoch 3, training loss 2.1817539310455323, training accuracy 28.75\n",
      "epoch 3, evaluating loss 2.2026934146881105, evaluating accuracy 26.0\n",
      "epoch 4, training loss 2.140290517807007, training accuracy 30.75\n",
      "epoch 4, evaluating loss 2.1780934715270996, evaluating accuracy 26.0\n",
      "epoch 5, training loss 2.117328748703003, training accuracy 33.5\n",
      "epoch 5, evaluating loss 2.145385971069336, evaluating accuracy 31.0\n",
      "epoch 6, training loss 2.0773104286193846, training accuracy 39.625\n",
      "epoch 6, evaluating loss 2.1117810916900637, evaluating accuracy 36.0\n",
      "epoch 7, training loss 2.060793609619141, training accuracy 42.625\n",
      "epoch 7, evaluating loss 2.1132136726379396, evaluating accuracy 36.5\n",
      "epoch 8, training loss 2.033462085723877, training accuracy 46.375\n",
      "epoch 8, evaluating loss 2.0644944667816163, evaluating accuracy 38.5\n",
      "epoch 9, training loss 2.008102493286133, training accuracy 48.5\n",
      "epoch 9, evaluating loss 2.0461391830444335, evaluating accuracy 45.5\n",
      "epoch 10, training loss 1.9822614192962646, training accuracy 51.75\n",
      "epoch 10, evaluating loss 2.027025375366211, evaluating accuracy 46.0\n",
      "epoch 11, training loss 1.961065306663513, training accuracy 55.125\n",
      "epoch 11, evaluating loss 2.0025174903869627, evaluating accuracy 46.5\n",
      "epoch 12, training loss 1.946338987350464, training accuracy 55.125\n",
      "epoch 12, evaluating loss 1.9966142225265502, evaluating accuracy 47.5\n",
      "epoch 13, training loss 1.929763069152832, training accuracy 56.5\n",
      "epoch 13, evaluating loss 1.9860381984710693, evaluating accuracy 49.0\n",
      "epoch 14, training loss 1.932821946144104, training accuracy 57.75\n",
      "epoch 14, evaluating loss 1.963930034637451, evaluating accuracy 50.5\n",
      "epoch 15, training loss 1.9111080932617188, training accuracy 58.5\n",
      "epoch 15, evaluating loss 1.937711009979248, evaluating accuracy 56.0\n",
      "epoch 16, training loss 1.8930517435073853, training accuracy 59.625\n",
      "epoch 16, evaluating loss 1.9421071529388427, evaluating accuracy 54.5\n",
      "epoch 17, training loss 1.8884961938858031, training accuracy 59.5\n",
      "epoch 17, evaluating loss 1.9034154319763184, evaluating accuracy 60.0\n",
      "epoch 18, training loss 1.8697600364685059, training accuracy 62.125\n",
      "epoch 18, evaluating loss 1.9193830251693726, evaluating accuracy 63.0\n",
      "epoch 19, training loss 1.8731686544418336, training accuracy 61.75\n",
      "epoch 19, evaluating loss 1.8853528642654418, evaluating accuracy 64.0\n",
      "Done training.\n",
      "Current length: 5\n",
      "device: cuda\n",
      "Warning: total_len is larger than the maximum possible length. \n",
      "Setting total_len to the maximum possible length. \n",
      "Warning: access length of dataset by len(dataset) to get the actual length. \n",
      "epoch 0, training loss 2.313872652053833, training accuracy 8.5\n",
      "epoch 0, evaluating loss 2.306339988708496, evaluating accuracy 10.5\n",
      "epoch 1, training loss 2.3080391597747805, training accuracy 9.75\n",
      "epoch 1, evaluating loss 2.320296449661255, evaluating accuracy 6.0\n",
      "epoch 2, training loss 2.3057229709625244, training accuracy 9.75\n",
      "epoch 2, evaluating loss 2.3224849319458007, evaluating accuracy 6.5\n",
      "epoch 3, training loss 2.3039537334442137, training accuracy 10.75\n",
      "epoch 3, evaluating loss 2.3164863204956054, evaluating accuracy 6.5\n",
      "epoch 4, training loss 2.2982288551330567, training accuracy 12.375\n",
      "epoch 4, evaluating loss 2.3001514148712157, evaluating accuracy 11.5\n",
      "epoch 5, training loss 2.252640838623047, training accuracy 18.125\n",
      "epoch 5, evaluating loss 2.2520576095581055, evaluating accuracy 17.5\n",
      "epoch 6, training loss 2.215327386856079, training accuracy 20.875\n",
      "epoch 6, evaluating loss 2.2455939865112304, evaluating accuracy 16.5\n",
      "epoch 7, training loss 2.193701467514038, training accuracy 26.125\n",
      "epoch 7, evaluating loss 2.2211468410491944, evaluating accuracy 21.5\n",
      "epoch 8, training loss 2.166558933258057, training accuracy 28.625\n",
      "epoch 8, evaluating loss 2.1866064834594727, evaluating accuracy 29.0\n",
      "epoch 9, training loss 2.125461235046387, training accuracy 36.125\n",
      "epoch 9, evaluating loss 2.1692236709594725, evaluating accuracy 29.0\n",
      "epoch 10, training loss 2.075860757827759, training accuracy 41.75\n",
      "epoch 10, evaluating loss 2.1521832180023193, evaluating accuracy 28.5\n",
      "epoch 11, training loss 2.0532322978973387, training accuracy 41.75\n",
      "epoch 11, evaluating loss 2.1413062477111815, evaluating accuracy 30.5\n",
      "epoch 12, training loss 2.042071976661682, training accuracy 42.5\n",
      "epoch 12, evaluating loss 2.144261779785156, evaluating accuracy 31.5\n",
      "epoch 13, training loss 2.0405161333084108, training accuracy 42.5\n",
      "epoch 13, evaluating loss 2.118126564025879, evaluating accuracy 33.0\n",
      "epoch 14, training loss 2.0290349197387694, training accuracy 40.5\n",
      "epoch 14, evaluating loss 2.1102319526672364, evaluating accuracy 29.0\n",
      "epoch 15, training loss 2.0228552341461183, training accuracy 41.875\n",
      "epoch 15, evaluating loss 2.1101478862762453, evaluating accuracy 30.0\n",
      "epoch 16, training loss 2.0176929235458374, training accuracy 42.5\n",
      "epoch 16, evaluating loss 2.107244710922241, evaluating accuracy 30.5\n",
      "epoch 17, training loss 2.016836709976196, training accuracy 42.25\n",
      "epoch 17, evaluating loss 2.1347155570983887, evaluating accuracy 28.5\n",
      "epoch 18, training loss 2.0221395683288574, training accuracy 44.125\n",
      "epoch 18, evaluating loss 2.08028039932251, evaluating accuracy 40.0\n",
      "epoch 19, training loss 1.9913121843338013, training accuracy 49.625\n",
      "epoch 19, evaluating loss 2.1033283424377442, evaluating accuracy 31.0\n",
      "Done training.\n",
      "Current length: 6\n",
      "device: cuda\n",
      "Warning: total_len is larger than the maximum possible length. \n",
      "Setting total_len to the maximum possible length. \n",
      "Warning: access length of dataset by len(dataset) to get the actual length. \n",
      "epoch 0, training loss 2.30713551902771, training accuracy 9.5\n",
      "epoch 0, evaluating loss 2.3051522331237795, evaluating accuracy 10.5\n",
      "epoch 1, training loss 2.2324531078338623, training accuracy 19.6125\n",
      "epoch 1, evaluating loss 2.1448383560180666, evaluating accuracy 29.0\n",
      "epoch 2, training loss 2.0957462978363037, training accuracy 34.8375\n",
      "epoch 2, evaluating loss 2.0554512176513673, evaluating accuracy 38.45\n",
      "epoch 3, training loss 2.026505309104919, training accuracy 42.0625\n",
      "epoch 3, evaluating loss 2.0028243198394775, evaluating accuracy 47.55\n",
      "epoch 4, training loss 1.9551105756759644, training accuracy 51.1125\n",
      "epoch 4, evaluating loss 1.9282042980194092, evaluating accuracy 55.85\n",
      "epoch 5, training loss 1.8866709480285644, training accuracy 58.7375\n",
      "epoch 5, evaluating loss 1.865802110671997, evaluating accuracy 58.7\n",
      "epoch 6, training loss 1.849707142829895, training accuracy 59.6875\n",
      "epoch 6, evaluating loss 1.8533833322525024, evaluating accuracy 58.7\n",
      "epoch 7, training loss 1.8766406764984132, training accuracy 56.875\n",
      "epoch 7, evaluating loss 1.9082492742538453, evaluating accuracy 54.2\n",
      "epoch 8, training loss 1.846318808555603, training accuracy 62.2125\n",
      "epoch 8, evaluating loss 1.798473258972168, evaluating accuracy 69.7\n",
      "epoch 9, training loss 1.7723736782073976, training accuracy 70.1\n",
      "epoch 9, evaluating loss 1.7649997959136963, evaluating accuracy 69.2\n",
      "epoch 10, training loss 1.7860543432235718, training accuracy 67.075\n",
      "epoch 10, evaluating loss 2.025818510055542, evaluating accuracy 42.25\n",
      "epoch 11, training loss 1.7611118869781495, training accuracy 72.3625\n",
      "epoch 11, evaluating loss 1.786691083908081, evaluating accuracy 68.0\n",
      "epoch 12, training loss 1.7137327547073364, training accuracy 77.35\n",
      "epoch 12, evaluating loss 1.678179048538208, evaluating accuracy 80.15\n",
      "epoch 13, training loss 1.7195041580200194, training accuracy 75.0875\n",
      "epoch 13, evaluating loss 1.6896110706329346, evaluating accuracy 80.2\n",
      "epoch 14, training loss 1.6788289394378662, training accuracy 82.6625\n",
      "epoch 14, evaluating loss 1.6221045551300048, evaluating accuracy 90.0\n",
      "epoch 15, training loss 1.6513331365585326, training accuracy 84.8125\n",
      "epoch 15, evaluating loss 1.5979982995986939, evaluating accuracy 90.0\n",
      "epoch 16, training loss 1.5890486469268799, training accuracy 90.0\n",
      "epoch 16, evaluating loss 1.600028200149536, evaluating accuracy 89.9\n",
      "epoch 17, training loss 1.6718663425445557, training accuracy 80.1\n",
      "epoch 17, evaluating loss 1.607742280960083, evaluating accuracy 88.9\n",
      "epoch 18, training loss 1.6004764146804809, training accuracy 87.5375\n",
      "epoch 18, evaluating loss 1.6439074096679687, evaluating accuracy 83.95\n",
      "epoch 19, training loss 1.5996608991622925, training accuracy 88.65\n",
      "epoch 19, evaluating loss 1.5393038873672484, evaluating accuracy 98.75\n",
      "Done training.\n",
      "Current length: 7\n",
      "device: cuda\n",
      "Warning: total_len is larger than the maximum possible length. \n",
      "Setting total_len to the maximum possible length. \n",
      "Warning: access length of dataset by len(dataset) to get the actual length. \n",
      "epoch 0, training loss 2.307529836654663, training accuracy 9.8\n",
      "epoch 0, evaluating loss 2.30472301864624, evaluating accuracy 9.95\n",
      "epoch 1, training loss 2.3047498207092287, training accuracy 9.3375\n",
      "epoch 1, evaluating loss 2.3047486686706544, evaluating accuracy 8.35\n",
      "epoch 2, training loss 2.304194053649902, training accuracy 9.5875\n",
      "epoch 2, evaluating loss 2.30479079246521, evaluating accuracy 8.4\n",
      "epoch 3, training loss 2.30401833152771, training accuracy 9.2\n",
      "epoch 3, evaluating loss 2.3048868370056153, evaluating accuracy 9.7\n",
      "epoch 4, training loss 2.2891308822631835, training accuracy 11.5875\n",
      "epoch 4, evaluating loss 2.2132945556640626, evaluating accuracy 19.9\n",
      "epoch 5, training loss 2.120980207443237, training accuracy 33.1375\n",
      "epoch 5, evaluating loss 2.037798225402832, evaluating accuracy 40.35\n",
      "epoch 6, training loss 1.9788086404800416, training accuracy 49.9375\n",
      "epoch 6, evaluating loss 1.9517077980041504, evaluating accuracy 50.4\n",
      "epoch 7, training loss 1.8822379560470581, training accuracy 59.0125\n",
      "epoch 7, evaluating loss 1.9423143920898438, evaluating accuracy 51.75\n",
      "epoch 8, training loss 1.8181578073501588, training accuracy 68.8125\n",
      "epoch 8, evaluating loss 1.7786976623535156, evaluating accuracy 74.8\n",
      "epoch 9, training loss 1.7361729793548584, training accuracy 79.5125\n",
      "epoch 9, evaluating loss 1.787083948135376, evaluating accuracy 62.3\n",
      "epoch 10, training loss 1.6829901094436646, training accuracy 83.5125\n",
      "epoch 10, evaluating loss 1.6165041742324828, evaluating accuracy 89.7\n",
      "epoch 11, training loss 1.625384295463562, training accuracy 88.5\n",
      "epoch 11, evaluating loss 1.606109610557556, evaluating accuracy 90.5\n",
      "epoch 12, training loss 1.5982579860687256, training accuracy 91.6\n",
      "epoch 12, evaluating loss 1.594348397254944, evaluating accuracy 89.25\n",
      "epoch 13, training loss 1.5756987152099609, training accuracy 92.5875\n",
      "epoch 13, evaluating loss 1.588779541015625, evaluating accuracy 91.05\n",
      "epoch 14, training loss 1.5673665475845338, training accuracy 92.825\n",
      "epoch 14, evaluating loss 1.5317505979537964, evaluating accuracy 99.7\n",
      "epoch 15, training loss 1.4968276529312134, training accuracy 99.1625\n",
      "epoch 15, evaluating loss 1.4802248373031617, evaluating accuracy 100.0\n",
      "epoch 16, training loss 1.4762892208099365, training accuracy 100.0\n",
      "epoch 16, evaluating loss 1.4736866788864136, evaluating accuracy 100.0\n",
      "epoch 17, training loss 1.4708520498275757, training accuracy 100.0\n",
      "epoch 17, evaluating loss 1.4706273460388184, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.5930057363510133, training accuracy 87.2375\n",
      "epoch 18, evaluating loss 1.4686972560882567, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.467141872406006, training accuracy 100.0\n",
      "epoch 19, evaluating loss 1.466307050704956, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 8\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3045485485076904, training accuracy 9.6575\n",
      "epoch 0, evaluating loss 2.304819369125366, evaluating accuracy 10.03\n",
      "epoch 1, training loss 2.303563701629639, training accuracy 10.1225\n",
      "epoch 1, evaluating loss 2.304260210800171, evaluating accuracy 9.52\n",
      "epoch 2, training loss 2.303474488067627, training accuracy 10.0275\n",
      "epoch 2, evaluating loss 2.3042217239379883, evaluating accuracy 9.97\n",
      "epoch 3, training loss 2.193545015716553, training accuracy 23.55\n",
      "epoch 3, evaluating loss 2.022130713272095, evaluating accuracy 42.54\n",
      "epoch 4, training loss 1.7805124965667725, training accuracy 71.1275\n",
      "epoch 4, evaluating loss 1.6492522102355958, evaluating accuracy 86.59\n",
      "epoch 5, training loss 1.5785464469909667, training accuracy 91.57\n",
      "epoch 5, evaluating loss 1.4778510734558106, evaluating accuracy 100.0\n",
      "epoch 6, training loss 1.5226881694793701, training accuracy 94.4825\n",
      "epoch 6, evaluating loss 1.9271086084365845, evaluating accuracy 53.52\n",
      "epoch 7, training loss 1.4795359212875365, training accuracy 98.3825\n",
      "epoch 7, evaluating loss 1.4622349765777587, evaluating accuracy 100.0\n",
      "epoch 8, training loss 1.4617395658493042, training accuracy 100.0\n",
      "epoch 8, evaluating loss 1.4614164764404296, evaluating accuracy 100.0\n",
      "epoch 9, training loss 1.461286266708374, training accuracy 100.0\n",
      "epoch 9, evaluating loss 1.4612072257995605, evaluating accuracy 100.0\n",
      "epoch 10, training loss 1.4611788076400758, training accuracy 100.0\n",
      "epoch 10, evaluating loss 1.461161950111389, evaluating accuracy 100.0\n",
      "epoch 11, training loss 1.4611560724258423, training accuracy 100.0\n",
      "epoch 11, evaluating loss 1.4611525079727172, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.4611514223098754, training accuracy 100.0\n",
      "epoch 12, evaluating loss 1.4611509820938111, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.461150861930847, training accuracy 100.0\n",
      "epoch 13, evaluating loss 1.4611507711410523, evaluating accuracy 100.0\n",
      "epoch 14, training loss 1.4611507665634156, training accuracy 100.0\n",
      "epoch 14, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.4611507654190063, training accuracy 100.0\n",
      "epoch 15, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 16, training loss 1.4611507654190063, training accuracy 100.0\n",
      "epoch 16, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 17, training loss 1.7685935647964477, training accuracy 69.205\n",
      "epoch 17, evaluating loss 1.8734482486724853, evaluating accuracy 58.62\n",
      "epoch 18, training loss 1.4984251831054687, training accuracy 96.2625\n",
      "epoch 18, evaluating loss 1.4611615169525147, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.4611567771911622, training accuracy 100.0\n",
      "epoch 19, evaluating loss 1.4611539335250854, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 9\n",
      "device: cuda\n",
      "epoch 0, training loss 2.1794464029312133, training accuracy 26.3025\n",
      "epoch 0, evaluating loss 1.8873228033065796, evaluating accuracy 62.11\n",
      "epoch 1, training loss 1.7857559844970703, training accuracy 71.6075\n",
      "epoch 1, evaluating loss 1.641886593437195, evaluating accuracy 88.88\n",
      "epoch 2, training loss 1.6561416379928589, training accuracy 82.4425\n",
      "epoch 2, evaluating loss 1.5675894985198975, evaluating accuracy 89.99\n",
      "epoch 3, training loss 1.611842880821228, training accuracy 85.03\n",
      "epoch 3, evaluating loss 1.5607826385498047, evaluating accuracy 89.99\n",
      "epoch 4, training loss 1.557846139717102, training accuracy 90.1575\n",
      "epoch 4, evaluating loss 1.5586339839935304, evaluating accuracy 89.99\n",
      "epoch 5, training loss 1.7152831720352173, training accuracy 74.4025\n",
      "epoch 5, evaluating loss 1.5551348051071168, evaluating accuracy 90.56\n",
      "epoch 6, training loss 1.6034425498962401, training accuracy 85.5875\n",
      "epoch 6, evaluating loss 1.5540362297058106, evaluating accuracy 90.56\n",
      "epoch 7, training loss 1.5593211616516114, training accuracy 89.905\n",
      "epoch 7, evaluating loss 1.5529605184555053, evaluating accuracy 90.56\n",
      "epoch 8, training loss 1.6178706413269044, training accuracy 84.1125\n",
      "epoch 8, evaluating loss 1.552937163925171, evaluating accuracy 90.56\n",
      "epoch 9, training loss 1.542057046508789, training accuracy 92.0825\n",
      "epoch 9, evaluating loss 1.464103157043457, evaluating accuracy 100.0\n",
      "epoch 10, training loss 1.4625869230270385, training accuracy 100.0\n",
      "epoch 10, evaluating loss 1.461717078590393, evaluating accuracy 100.0\n",
      "epoch 11, training loss 1.461443119430542, training accuracy 100.0\n",
      "epoch 11, evaluating loss 1.4612705955505372, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.4612120552062988, training accuracy 100.0\n",
      "epoch 12, evaluating loss 1.4611754440307618, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.4611629707336427, training accuracy 100.0\n",
      "epoch 13, evaluating loss 1.4611553884506225, evaluating accuracy 100.0\n",
      "epoch 14, training loss 1.4611527368545532, training accuracy 100.0\n",
      "epoch 14, evaluating loss 1.4611512424468993, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.4611509885787963, training accuracy 100.0\n",
      "epoch 15, evaluating loss 1.461150872039795, evaluating accuracy 100.0\n",
      "epoch 16, training loss 1.4611507928848266, training accuracy 100.0\n",
      "epoch 16, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 17, training loss 1.4611507654190063, training accuracy 100.0\n",
      "epoch 17, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.4611507654190063, training accuracy 100.0\n",
      "epoch 18, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.4611507654190063, training accuracy 100.0\n",
      "epoch 19, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 10\n",
      "device: cuda\n",
      "epoch 0, training loss 2.304054956436157, training accuracy 10.21\n",
      "epoch 0, evaluating loss 2.3035509811401367, evaluating accuracy 10.49\n",
      "epoch 1, training loss 2.303334160232544, training accuracy 10.035\n",
      "epoch 1, evaluating loss 2.3033831146240233, evaluating accuracy 10.5\n",
      "epoch 2, training loss 2.302902716064453, training accuracy 10.2825\n",
      "epoch 2, evaluating loss 2.3040341815948486, evaluating accuracy 9.89\n",
      "epoch 3, training loss 2.128553399467468, training accuracy 30.915\n",
      "epoch 3, evaluating loss 1.9339467433929443, evaluating accuracy 54.96\n",
      "epoch 4, training loss 1.8163703651428222, training accuracy 66.8875\n",
      "epoch 4, evaluating loss 1.6644119535446167, evaluating accuracy 89.99\n",
      "epoch 5, training loss 1.6325036142349243, training accuracy 85.4275\n",
      "epoch 5, evaluating loss 1.5665997844696045, evaluating accuracy 90.08\n",
      "epoch 6, training loss 1.6087309761047364, training accuracy 85.665\n",
      "epoch 6, evaluating loss 1.5284804372787475, evaluating accuracy 95.9\n",
      "epoch 7, training loss 1.4685913991928101, training accuracy 99.815\n",
      "epoch 7, evaluating loss 1.4626893032073975, evaluating accuracy 100.0\n",
      "epoch 8, training loss 1.461880271911621, training accuracy 100.0\n",
      "epoch 8, evaluating loss 1.4614228736877442, evaluating accuracy 100.0\n",
      "epoch 9, training loss 1.4612804203033447, training accuracy 100.0\n",
      "epoch 9, evaluating loss 1.4612010305404664, evaluating accuracy 100.0\n",
      "epoch 10, training loss 1.4611753170013428, training accuracy 100.0\n",
      "epoch 10, evaluating loss 1.461160139656067, evaluating accuracy 100.0\n",
      "epoch 11, training loss 1.461155076599121, training accuracy 100.0\n",
      "epoch 11, evaluating loss 1.4611520570755006, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.4611512477874755, training accuracy 100.0\n",
      "epoch 12, evaluating loss 1.461150931739807, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.6058587106704711, training accuracy 85.51\n",
      "epoch 13, evaluating loss 1.7618918689727783, evaluating accuracy 69.88\n",
      "epoch 14, training loss 1.626978973388672, training accuracy 83.395\n",
      "epoch 14, evaluating loss 1.4611716737747193, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.461165922164917, training accuracy 100.0\n",
      "epoch 15, evaluating loss 1.4611608554840088, evaluating accuracy 100.0\n",
      "epoch 16, training loss 1.4611569829940796, training accuracy 100.0\n",
      "epoch 16, evaluating loss 1.4611542043685912, evaluating accuracy 100.0\n",
      "epoch 17, training loss 1.4611525695800782, training accuracy 100.0\n",
      "epoch 17, evaluating loss 1.4611515018463135, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.544754526901245, training accuracy 91.6425\n",
      "epoch 18, evaluating loss 1.7812629192352294, evaluating accuracy 67.84\n",
      "epoch 19, training loss 1.490521378326416, training accuracy 97.0725\n",
      "epoch 19, evaluating loss 1.4611521417617799, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 11\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3044103256225585, training accuracy 9.86\n",
      "epoch 0, evaluating loss 2.3035649150848387, evaluating accuracy 9.8\n",
      "epoch 1, training loss 2.3035572563171387, training accuracy 9.8075\n",
      "epoch 1, evaluating loss 2.3033449169158935, evaluating accuracy 10.56\n",
      "epoch 2, training loss 2.3029967021942137, training accuracy 10.2925\n",
      "epoch 2, evaluating loss 2.3042644660949705, evaluating accuracy 10.2\n",
      "epoch 3, training loss 2.2379430519104004, training accuracy 19.13\n",
      "epoch 3, evaluating loss 1.9729017711639405, evaluating accuracy 49.47\n",
      "epoch 4, training loss 1.8203709728240967, training accuracy 67.265\n",
      "epoch 4, evaluating loss 1.6751339736938478, evaluating accuracy 79.66\n",
      "epoch 5, training loss 1.6611984785079956, training accuracy 81.26\n",
      "epoch 5, evaluating loss 1.5683355207443237, evaluating accuracy 89.86\n",
      "epoch 6, training loss 1.616259098815918, training accuracy 84.665\n",
      "epoch 6, evaluating loss 1.5625795238494873, evaluating accuracy 89.86\n",
      "epoch 7, training loss 1.576081950378418, training accuracy 89.5275\n",
      "epoch 7, evaluating loss 1.467110318183899, evaluating accuracy 100.0\n",
      "epoch 8, training loss 1.4637178424835204, training accuracy 100.0\n",
      "epoch 8, evaluating loss 1.4620500198364257, evaluating accuracy 100.0\n",
      "epoch 9, training loss 1.461572452545166, training accuracy 100.0\n",
      "epoch 9, evaluating loss 1.461310217857361, evaluating accuracy 100.0\n",
      "epoch 10, training loss 1.4612281600952148, training accuracy 100.0\n",
      "epoch 10, evaluating loss 1.4611813306808472, evaluating accuracy 100.0\n",
      "epoch 11, training loss 1.4611656196594238, training accuracy 100.0\n",
      "epoch 11, evaluating loss 1.461156419944763, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.4611532346725464, training accuracy 100.0\n",
      "epoch 12, evaluating loss 1.461151384162903, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.4611510364532472, training accuracy 100.0\n",
      "epoch 13, evaluating loss 1.4611508750915527, evaluating accuracy 100.0\n",
      "epoch 14, training loss 1.4611507970809936, training accuracy 100.0\n",
      "epoch 14, evaluating loss 1.4611507650375366, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.6952749557495117, training accuracy 76.5725\n",
      "epoch 15, evaluating loss 2.254684952926636, evaluating accuracy 20.62\n",
      "epoch 16, training loss 2.129849919128418, training accuracy 33.0675\n",
      "epoch 16, evaluating loss 1.861073164367676, evaluating accuracy 59.99\n",
      "epoch 17, training loss 1.4866648374557496, training accuracy 97.46\n",
      "epoch 17, evaluating loss 1.4611628644943238, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.8058146335601806, training accuracy 65.4275\n",
      "epoch 18, evaluating loss 1.9568729400634766, evaluating accuracy 50.38\n",
      "epoch 19, training loss 1.6100789712905883, training accuracy 85.09\n",
      "epoch 19, evaluating loss 1.4611701862335205, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 12\n",
      "device: cuda\n",
      "epoch 0, training loss 2.304398348617554, training accuracy 9.845\n",
      "epoch 0, evaluating loss 2.3041226627349856, evaluating accuracy 9.71\n",
      "epoch 1, training loss 2.303209100341797, training accuracy 10.155\n",
      "epoch 1, evaluating loss 2.3038831729888916, evaluating accuracy 9.8\n",
      "epoch 2, training loss 2.303119295501709, training accuracy 10.19\n",
      "epoch 2, evaluating loss 2.303284148025513, evaluating accuracy 10.57\n",
      "epoch 3, training loss 2.3028810832977293, training accuracy 10.215\n",
      "epoch 3, evaluating loss 2.3032407287597656, evaluating accuracy 10.42\n",
      "epoch 4, training loss 2.3027693607330324, training accuracy 10.12\n",
      "epoch 4, evaluating loss 2.304045747375488, evaluating accuracy 9.77\n",
      "epoch 5, training loss 2.185167597579956, training accuracy 25.54\n",
      "epoch 5, evaluating loss 1.8671683330535889, evaluating accuracy 60.53\n",
      "epoch 6, training loss 1.7462536205291748, training accuracy 76.485\n",
      "epoch 6, evaluating loss 1.6167410898208618, evaluating accuracy 88.43\n",
      "epoch 7, training loss 1.5360834997177124, training accuracy 95.5825\n",
      "epoch 7, evaluating loss 1.4718904180526733, evaluating accuracy 100.0\n",
      "epoch 8, training loss 1.5408475654602052, training accuracy 92.5225\n",
      "epoch 8, evaluating loss 1.464327121925354, evaluating accuracy 100.0\n",
      "epoch 9, training loss 1.462866926574707, training accuracy 100.0\n",
      "epoch 9, evaluating loss 1.4619994781494141, evaluating accuracy 100.0\n",
      "epoch 10, training loss 1.4616066764831543, training accuracy 100.0\n",
      "epoch 10, evaluating loss 1.4613480012893676, evaluating accuracy 100.0\n",
      "epoch 11, training loss 1.4612499500274658, training accuracy 100.0\n",
      "epoch 11, evaluating loss 1.4611907133102418, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.4611701345443726, training accuracy 100.0\n",
      "epoch 12, evaluating loss 1.4611581163406373, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.4611540767669677, training accuracy 100.0\n",
      "epoch 13, evaluating loss 1.4611516162872313, evaluating accuracy 100.0\n",
      "epoch 14, training loss 1.4611511400222779, training accuracy 100.0\n",
      "epoch 14, evaluating loss 1.461150902748108, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.6330468301773071, training accuracy 82.79\n",
      "epoch 15, evaluating loss 1.72500305519104, evaluating accuracy 73.57\n",
      "epoch 16, training loss 1.5364984680175782, training accuracy 92.465\n",
      "epoch 16, evaluating loss 1.591048339653015, evaluating accuracy 86.98\n",
      "epoch 17, training loss 1.4937015983581543, training accuracy 96.7425\n",
      "epoch 17, evaluating loss 1.4611580961227417, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.4611543043136597, training accuracy 100.0\n",
      "epoch 18, evaluating loss 1.4611552572250366, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.4611519424438477, training accuracy 100.0\n",
      "epoch 19, evaluating loss 1.461151385498047, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 13\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3041214767456055, training accuracy 10.2775\n",
      "epoch 0, evaluating loss 2.3047552337646486, evaluating accuracy 10.18\n",
      "epoch 1, training loss 2.3032068641662597, training accuracy 10.06\n",
      "epoch 1, evaluating loss 2.304327820587158, evaluating accuracy 9.54\n",
      "epoch 2, training loss 2.302752711868286, training accuracy 10.2625\n",
      "epoch 2, evaluating loss 2.304605788040161, evaluating accuracy 10.05\n",
      "epoch 3, training loss 2.302443017578125, training accuracy 10.6925\n",
      "epoch 3, evaluating loss 2.304411388015747, evaluating accuracy 9.7\n",
      "epoch 4, training loss 2.302009976196289, training accuracy 10.83\n",
      "epoch 4, evaluating loss 2.3039561714172363, evaluating accuracy 10.54\n",
      "epoch 5, training loss 2.3018289321899412, training accuracy 10.645\n",
      "epoch 5, evaluating loss 2.304471449661255, evaluating accuracy 9.76\n",
      "epoch 6, training loss 2.1864868335723875, training accuracy 23.8825\n",
      "epoch 6, evaluating loss 2.1216722007751465, evaluating accuracy 30.37\n",
      "epoch 7, training loss 2.046871915626526, training accuracy 38.8275\n",
      "epoch 7, evaluating loss 2.0309765781402587, evaluating accuracy 39.98\n",
      "epoch 8, training loss 1.9712715688705444, training accuracy 47.84\n",
      "epoch 8, evaluating loss 2.0830338020324706, evaluating accuracy 35.05\n",
      "epoch 9, training loss 1.7432036449432373, training accuracy 74.285\n",
      "epoch 9, evaluating loss 1.563013826560974, evaluating accuracy 90.37\n",
      "epoch 10, training loss 1.600997537612915, training accuracy 87.1025\n",
      "epoch 10, evaluating loss 1.5651110919952393, evaluating accuracy 90.28\n",
      "epoch 11, training loss 1.545084642982483, training accuracy 92.735\n",
      "epoch 11, evaluating loss 1.4703989974975586, evaluating accuracy 100.0\n",
      "epoch 12, training loss 1.5186145906448365, training accuracy 94.6625\n",
      "epoch 12, evaluating loss 1.4637458915710448, evaluating accuracy 99.99\n",
      "epoch 13, training loss 1.5721652894973754, training accuracy 89.0225\n",
      "epoch 13, evaluating loss 1.4637388460159302, evaluating accuracy 99.94\n",
      "epoch 14, training loss 1.4791144258499145, training accuracy 98.305\n",
      "epoch 14, evaluating loss 1.4616719520568848, evaluating accuracy 100.0\n",
      "epoch 15, training loss 1.4614416603088378, training accuracy 100.0\n",
      "epoch 15, evaluating loss 1.4612890640258789, evaluating accuracy 100.0\n",
      "epoch 16, training loss 1.656332972717285, training accuracy 80.4375\n",
      "epoch 16, evaluating loss 1.96572264251709, evaluating accuracy 49.15\n",
      "epoch 17, training loss 1.8003546829223633, training accuracy 65.8275\n",
      "epoch 17, evaluating loss 1.6673648622512818, evaluating accuracy 79.43\n",
      "epoch 18, training loss 1.6610805522918701, training accuracy 80.025\n",
      "epoch 18, evaluating loss 1.6669000408172607, evaluating accuracy 79.43\n",
      "epoch 19, training loss 1.6597537609100341, training accuracy 80.1425\n",
      "epoch 19, evaluating loss 1.6668660398483277, evaluating accuracy 79.43\n",
      "Done training.\n",
      "Current length: 14\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3042478958129884, training accuracy 9.805\n",
      "epoch 0, evaluating loss 2.3031237689971924, evaluating accuracy 10.24\n",
      "epoch 1, training loss 2.3032391429901122, training accuracy 10.1125\n",
      "epoch 1, evaluating loss 2.3041079833984375, evaluating accuracy 9.9\n",
      "epoch 2, training loss 2.3031315086364748, training accuracy 10.0975\n",
      "epoch 2, evaluating loss 2.3040663944244386, evaluating accuracy 10.04\n",
      "epoch 3, training loss 2.302765839767456, training accuracy 10.4425\n",
      "epoch 3, evaluating loss 2.304042608642578, evaluating accuracy 10.14\n",
      "epoch 4, training loss 2.302651953125, training accuracy 10.2625\n",
      "epoch 4, evaluating loss 2.304526774597168, evaluating accuracy 9.85\n",
      "epoch 5, training loss 2.3024149688720703, training accuracy 10.4525\n",
      "epoch 5, evaluating loss 2.3049086074829104, evaluating accuracy 9.69\n",
      "epoch 6, training loss 2.3021189571380614, training accuracy 10.6225\n",
      "epoch 6, evaluating loss 2.3033088863372804, evaluating accuracy 10.19\n",
      "epoch 7, training loss 2.301838173675537, training accuracy 10.65\n",
      "epoch 7, evaluating loss 2.303970658874512, evaluating accuracy 9.86\n",
      "epoch 8, training loss 2.2606308822631838, training accuracy 15.5\n",
      "epoch 8, evaluating loss 2.212561409378052, evaluating accuracy 19.82\n",
      "epoch 9, training loss 2.211305773925781, training accuracy 20.16\n",
      "epoch 9, evaluating loss 2.2139657821655274, evaluating accuracy 19.43\n",
      "epoch 10, training loss 2.210052037811279, training accuracy 19.9675\n",
      "epoch 10, evaluating loss 2.2105935230255125, evaluating accuracy 19.96\n",
      "epoch 11, training loss 2.20919310836792, training accuracy 20.6\n",
      "epoch 11, evaluating loss 2.2111030708312986, evaluating accuracy 19.37\n",
      "epoch 12, training loss 2.209773398590088, training accuracy 20.235\n",
      "epoch 12, evaluating loss 2.21234475402832, evaluating accuracy 19.77\n",
      "epoch 13, training loss 2.2096430511474607, training accuracy 20.375\n",
      "epoch 13, evaluating loss 2.211051240539551, evaluating accuracy 19.67\n",
      "epoch 14, training loss 2.2089618576049803, training accuracy 20.225\n",
      "epoch 14, evaluating loss 2.212005928039551, evaluating accuracy 19.62\n",
      "epoch 15, training loss 2.2091700653076174, training accuracy 20.2425\n",
      "epoch 15, evaluating loss 2.2109840591430663, evaluating accuracy 19.93\n",
      "epoch 16, training loss 2.208534465408325, training accuracy 20.3925\n",
      "epoch 16, evaluating loss 2.2114537586212157, evaluating accuracy 19.8\n",
      "epoch 17, training loss 2.208139262008667, training accuracy 20.725\n",
      "epoch 17, evaluating loss 2.211687775039673, evaluating accuracy 19.48\n",
      "epoch 18, training loss 2.2081205322265625, training accuracy 20.6875\n",
      "epoch 18, evaluating loss 2.210863327026367, evaluating accuracy 20.11\n",
      "epoch 19, training loss 2.189378902053833, training accuracy 22.7925\n",
      "epoch 19, evaluating loss 2.1218465438842773, evaluating accuracy 29.45\n",
      "Done training.\n",
      "Current length: 15\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3044980407714846, training accuracy 10.0225\n",
      "epoch 0, evaluating loss 2.30383532371521, evaluating accuracy 9.9\n",
      "epoch 1, training loss 2.303122832107544, training accuracy 10.3225\n",
      "epoch 1, evaluating loss 2.305037561416626, evaluating accuracy 9.44\n",
      "epoch 2, training loss 2.3029655448913573, training accuracy 10.105\n",
      "epoch 2, evaluating loss 2.303992855453491, evaluating accuracy 9.54\n",
      "epoch 3, training loss 2.30278284034729, training accuracy 10.3775\n",
      "epoch 3, evaluating loss 2.3057695442199706, evaluating accuracy 9.35\n",
      "epoch 4, training loss 2.302370488357544, training accuracy 10.555\n",
      "epoch 4, evaluating loss 2.3037831378936766, evaluating accuracy 10.32\n",
      "epoch 5, training loss 2.302057946395874, training accuracy 10.5475\n",
      "epoch 5, evaluating loss 2.3036699310302735, evaluating accuracy 10.34\n",
      "epoch 6, training loss 2.3016935218811034, training accuracy 10.755\n",
      "epoch 6, evaluating loss 2.3047506217956544, evaluating accuracy 9.82\n",
      "epoch 7, training loss 2.2767858013153077, training accuracy 14.05\n",
      "epoch 7, evaluating loss 2.0854243114471434, evaluating accuracy 37.46\n",
      "epoch 8, training loss 1.853509844970703, training accuracy 63.3625\n",
      "epoch 8, evaluating loss 1.8271750003814697, evaluating accuracy 62.54\n",
      "epoch 9, training loss 1.6705480712890626, training accuracy 82.18\n",
      "epoch 9, evaluating loss 1.833475632095337, evaluating accuracy 60.75\n",
      "epoch 10, training loss 1.5445499197006225, training accuracy 93.415\n",
      "epoch 10, evaluating loss 1.9218878238677979, evaluating accuracy 53.24\n",
      "epoch 11, training loss 1.5434505161285401, training accuracy 92.23\n",
      "epoch 11, evaluating loss 1.4976130409240722, evaluating accuracy 97.03\n",
      "epoch 12, training loss 1.4746138233184813, training accuracy 98.8675\n",
      "epoch 12, evaluating loss 1.4620635400772095, evaluating accuracy 100.0\n",
      "epoch 13, training loss 1.4616515373229981, training accuracy 100.0\n",
      "epoch 13, evaluating loss 1.4613762920379638, evaluating accuracy 100.0\n",
      "epoch 14, training loss 1.5737765693664552, training accuracy 88.7125\n",
      "epoch 14, evaluating loss 1.8962497993469238, evaluating accuracy 56.61\n",
      "epoch 15, training loss 1.6846375699996947, training accuracy 77.69\n",
      "epoch 15, evaluating loss 1.6613346574783325, evaluating accuracy 80.28\n",
      "epoch 16, training loss 1.5683812404632569, training accuracy 89.33\n",
      "epoch 16, evaluating loss 1.46288099193573, evaluating accuracy 99.86\n",
      "epoch 17, training loss 1.6309542505264283, training accuracy 83.025\n",
      "epoch 17, evaluating loss 1.6541351947784424, evaluating accuracy 80.69\n",
      "epoch 18, training loss 1.6228138444900513, training accuracy 83.8525\n",
      "epoch 18, evaluating loss 1.46142462348938, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.56673613986969, training accuracy 89.4625\n",
      "epoch 19, evaluating loss 1.4691817264556886, evaluating accuracy 99.23\n",
      "Done training.\n",
      "Current length: 16\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3046174209594725, training accuracy 9.8325\n",
      "epoch 0, evaluating loss 2.3032586574554443, evaluating accuracy 9.74\n",
      "epoch 1, training loss 2.3032797061920167, training accuracy 10.1375\n",
      "epoch 1, evaluating loss 2.3034121196746824, evaluating accuracy 10.13\n",
      "epoch 2, training loss 2.3029714431762693, training accuracy 10.145\n",
      "epoch 2, evaluating loss 2.3043906005859376, evaluating accuracy 10.28\n",
      "epoch 3, training loss 2.3029897926330567, training accuracy 10.28\n",
      "epoch 3, evaluating loss 2.3034734798431398, evaluating accuracy 10.08\n",
      "epoch 4, training loss 2.302758634185791, training accuracy 10.235\n",
      "epoch 4, evaluating loss 2.3039656272888185, evaluating accuracy 10.33\n",
      "epoch 5, training loss 2.302714019012451, training accuracy 10.4975\n",
      "epoch 5, evaluating loss 2.304794874572754, evaluating accuracy 9.3\n",
      "epoch 6, training loss 2.3025959491729737, training accuracy 10.4825\n",
      "epoch 6, evaluating loss 2.3041756488800047, evaluating accuracy 9.64\n",
      "epoch 7, training loss 2.302186280822754, training accuracy 10.615\n",
      "epoch 7, evaluating loss 2.3040291133880615, evaluating accuracy 9.96\n",
      "epoch 8, training loss 2.3018388832092285, training accuracy 10.96\n",
      "epoch 8, evaluating loss 2.3060261993408204, evaluating accuracy 9.6\n",
      "epoch 9, training loss 2.301888555908203, training accuracy 10.62\n",
      "epoch 9, evaluating loss 2.303745930862427, evaluating accuracy 10.08\n",
      "epoch 10, training loss 2.301555676269531, training accuracy 10.8275\n",
      "epoch 10, evaluating loss 2.3069067249298096, evaluating accuracy 9.65\n",
      "epoch 11, training loss 2.301243495178223, training accuracy 10.7725\n",
      "epoch 11, evaluating loss 2.30396378288269, evaluating accuracy 9.93\n",
      "epoch 12, training loss 2.278676671600342, training accuracy 14.825\n",
      "epoch 12, evaluating loss 2.080575340270996, evaluating accuracy 39.82\n",
      "epoch 13, training loss 1.9060201459884643, training accuracy 58.665\n",
      "epoch 13, evaluating loss 1.770461835670471, evaluating accuracy 70.75\n",
      "epoch 14, training loss 1.7602612926483154, training accuracy 72.465\n",
      "epoch 14, evaluating loss 1.590726022529602, evaluating accuracy 90.41\n",
      "epoch 15, training loss 1.6142637092590333, training accuracy 85.895\n",
      "epoch 15, evaluating loss 1.792624798965454, evaluating accuracy 66.93\n",
      "epoch 16, training loss 1.6120617551803589, training accuracy 85.0225\n",
      "epoch 16, evaluating loss 1.5559142110824584, evaluating accuracy 90.41\n",
      "epoch 17, training loss 1.5803813694000244, training accuracy 89.1575\n",
      "epoch 17, evaluating loss 1.4683718124389649, evaluating accuracy 100.0\n",
      "epoch 18, training loss 1.52187353515625, training accuracy 94.3525\n",
      "epoch 18, evaluating loss 1.4636059078216552, evaluating accuracy 100.0\n",
      "epoch 19, training loss 1.4626563940048218, training accuracy 100.0\n",
      "epoch 19, evaluating loss 1.461898861503601, evaluating accuracy 100.0\n",
      "Done training.\n",
      "Current length: 17\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3044354511260985, training accuracy 9.9025\n",
      "epoch 0, evaluating loss 2.3030850616455076, evaluating accuracy 9.93\n",
      "epoch 1, training loss 2.3035029327392578, training accuracy 9.935\n",
      "epoch 1, evaluating loss 2.302965689086914, evaluating accuracy 10.11\n",
      "epoch 2, training loss 2.3029110260009764, training accuracy 10.355\n",
      "epoch 2, evaluating loss 2.3034149665832517, evaluating accuracy 9.79\n",
      "epoch 3, training loss 2.302645125579834, training accuracy 10.4325\n",
      "epoch 3, evaluating loss 2.3035504333496095, evaluating accuracy 9.71\n",
      "epoch 4, training loss 2.302485570144653, training accuracy 10.3975\n",
      "epoch 4, evaluating loss 2.30315209274292, evaluating accuracy 10.62\n",
      "epoch 5, training loss 2.3019508289337156, training accuracy 10.7675\n",
      "epoch 5, evaluating loss 2.303688261413574, evaluating accuracy 10.17\n",
      "epoch 6, training loss 2.301604893875122, training accuracy 10.78\n",
      "epoch 6, evaluating loss 2.3039333015441894, evaluating accuracy 10.0\n",
      "epoch 7, training loss 2.3013402404785155, training accuracy 10.8725\n",
      "epoch 7, evaluating loss 2.3056916172027586, evaluating accuracy 9.86\n",
      "epoch 8, training loss 2.301046970748901, training accuracy 11.055\n",
      "epoch 8, evaluating loss 2.3035929824829102, evaluating accuracy 10.3\n",
      "epoch 9, training loss 2.2771393878936768, training accuracy 13.985\n",
      "epoch 9, evaluating loss 2.220732279586792, evaluating accuracy 20.09\n",
      "epoch 10, training loss 2.2112816421508787, training accuracy 20.005\n",
      "epoch 10, evaluating loss 2.207316897583008, evaluating accuracy 20.18\n",
      "epoch 11, training loss 2.211736823654175, training accuracy 20.1\n",
      "epoch 11, evaluating loss 2.211326071548462, evaluating accuracy 19.64\n",
      "epoch 12, training loss 2.2207953975677492, training accuracy 19.96\n",
      "epoch 12, evaluating loss 2.214045922470093, evaluating accuracy 19.92\n",
      "epoch 13, training loss 2.2104116146087645, training accuracy 20.5925\n",
      "epoch 13, evaluating loss 2.2090360301971437, evaluating accuracy 20.62\n",
      "epoch 14, training loss 2.2110820232391357, training accuracy 20.7825\n",
      "epoch 14, evaluating loss 2.2076120571136473, evaluating accuracy 20.68\n",
      "epoch 15, training loss 2.219976411437988, training accuracy 19.95\n",
      "epoch 15, evaluating loss 2.2835273529052733, evaluating accuracy 13.16\n",
      "epoch 16, training loss 2.221956715774536, training accuracy 19.7425\n",
      "epoch 16, evaluating loss 2.2149200626373293, evaluating accuracy 20.23\n",
      "epoch 17, training loss 2.2124902866363527, training accuracy 20.82\n",
      "epoch 17, evaluating loss 2.216415575790405, evaluating accuracy 19.67\n",
      "epoch 18, training loss 2.288291002655029, training accuracy 13.0425\n",
      "epoch 18, evaluating loss 2.2960674419403078, evaluating accuracy 11.57\n",
      "epoch 19, training loss 2.237628694152832, training accuracy 18.1925\n",
      "epoch 19, evaluating loss 2.231529195022583, evaluating accuracy 18.6\n",
      "Done training.\n",
      "Current length: 18\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3042576866149904, training accuracy 9.715\n",
      "epoch 0, evaluating loss 2.302976235580444, evaluating accuracy 10.21\n",
      "epoch 1, training loss 2.303158801651001, training accuracy 10.23\n",
      "epoch 1, evaluating loss 2.304060773086548, evaluating accuracy 9.63\n",
      "epoch 2, training loss 2.3029109493255615, training accuracy 10.0775\n",
      "epoch 2, evaluating loss 2.3037765407562256, evaluating accuracy 10.04\n",
      "epoch 3, training loss 2.3027267452239992, training accuracy 10.31\n",
      "epoch 3, evaluating loss 2.303629502105713, evaluating accuracy 10.06\n",
      "epoch 4, training loss 2.302195555114746, training accuracy 10.7225\n",
      "epoch 4, evaluating loss 2.304262215423584, evaluating accuracy 9.78\n",
      "epoch 5, training loss 2.2763075881958006, training accuracy 13.705\n",
      "epoch 5, evaluating loss 2.2248176910400392, evaluating accuracy 18.92\n",
      "epoch 6, training loss 2.224122729110718, training accuracy 18.7825\n",
      "epoch 6, evaluating loss 2.2153906787872315, evaluating accuracy 19.56\n",
      "epoch 7, training loss 2.2121918685913085, training accuracy 20.0\n",
      "epoch 7, evaluating loss 2.207667453765869, evaluating accuracy 20.74\n",
      "epoch 8, training loss 2.2122513343811034, training accuracy 20.0925\n",
      "epoch 8, evaluating loss 2.2142201015472414, evaluating accuracy 19.84\n",
      "epoch 9, training loss 2.2123580940246583, training accuracy 20.015\n",
      "epoch 9, evaluating loss 2.2075038913726805, evaluating accuracy 20.5\n",
      "epoch 10, training loss 2.2148985401153563, training accuracy 19.9\n",
      "epoch 10, evaluating loss 2.2123117191314696, evaluating accuracy 20.09\n",
      "epoch 11, training loss 2.2122715923309326, training accuracy 20.2375\n",
      "epoch 11, evaluating loss 2.207068409347534, evaluating accuracy 20.59\n",
      "epoch 12, training loss 2.2092396591186523, training accuracy 20.5475\n",
      "epoch 12, evaluating loss 2.2083303398132323, evaluating accuracy 20.25\n",
      "epoch 13, training loss 2.2093360317230224, training accuracy 20.655\n",
      "epoch 13, evaluating loss 2.2072770763397216, evaluating accuracy 20.23\n",
      "epoch 14, training loss 2.25452880897522, training accuracy 17.5075\n",
      "epoch 14, evaluating loss 2.351225905227661, evaluating accuracy 11.02\n",
      "epoch 15, training loss 2.3792621017456055, training accuracy 8.155\n",
      "epoch 15, evaluating loss 2.384316696548462, evaluating accuracy 7.68\n",
      "epoch 16, training loss 2.3865290187835693, training accuracy 7.4225\n",
      "epoch 16, evaluating loss 2.376523557281494, evaluating accuracy 8.4\n",
      "epoch 17, training loss 2.3805940406799317, training accuracy 7.9725\n",
      "epoch 17, evaluating loss 2.3767659675598143, evaluating accuracy 8.34\n",
      "epoch 18, training loss 2.3795532436370848, training accuracy 8.06\n",
      "epoch 18, evaluating loss 2.374601128387451, evaluating accuracy 8.55\n",
      "epoch 19, training loss 2.3779450870513914, training accuracy 8.2275\n",
      "epoch 19, evaluating loss 2.3575508842468262, evaluating accuracy 10.36\n",
      "Done training.\n",
      "Current length: 19\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3041591789245603, training accuracy 9.8925\n",
      "epoch 0, evaluating loss 2.3048311157226564, evaluating accuracy 9.38\n",
      "epoch 1, training loss 2.3027846405029297, training accuracy 10.425\n",
      "epoch 1, evaluating loss 2.3058533699035646, evaluating accuracy 10.15\n",
      "epoch 2, training loss 2.3028610118865966, training accuracy 10.335\n",
      "epoch 2, evaluating loss 2.3032629249572754, evaluating accuracy 9.97\n",
      "epoch 3, training loss 2.302396726989746, training accuracy 10.4525\n",
      "epoch 3, evaluating loss 2.303590943145752, evaluating accuracy 10.19\n",
      "epoch 4, training loss 2.3022743309020997, training accuracy 10.375\n",
      "epoch 4, evaluating loss 2.3033148040771483, evaluating accuracy 9.84\n",
      "epoch 5, training loss 2.301931699371338, training accuracy 10.655\n",
      "epoch 5, evaluating loss 2.30347663192749, evaluating accuracy 10.19\n",
      "epoch 6, training loss 2.3015374420166017, training accuracy 10.815\n",
      "epoch 6, evaluating loss 2.303661015319824, evaluating accuracy 10.14\n",
      "epoch 7, training loss 2.3011836380004884, training accuracy 10.8925\n",
      "epoch 7, evaluating loss 2.303078131103516, evaluating accuracy 10.15\n",
      "epoch 8, training loss 2.3008428325653076, training accuracy 11.12\n",
      "epoch 8, evaluating loss 2.3059058437347413, evaluating accuracy 9.83\n",
      "epoch 9, training loss 2.3007550407409667, training accuracy 11.0325\n",
      "epoch 9, evaluating loss 2.303966450881958, evaluating accuracy 10.18\n",
      "epoch 10, training loss 2.300108469390869, training accuracy 11.3325\n",
      "epoch 10, evaluating loss 2.3039805950164793, evaluating accuracy 10.06\n",
      "epoch 11, training loss 2.2996248455047605, training accuracy 11.4325\n",
      "epoch 11, evaluating loss 2.3040197261810302, evaluating accuracy 10.01\n",
      "epoch 12, training loss 2.2991477249145507, training accuracy 11.65\n",
      "epoch 12, evaluating loss 2.3045221366882322, evaluating accuracy 10.35\n",
      "epoch 13, training loss 2.298306314468384, training accuracy 11.825\n",
      "epoch 13, evaluating loss 2.3042228858947755, evaluating accuracy 10.3\n",
      "epoch 14, training loss 2.297585530471802, training accuracy 11.9475\n",
      "epoch 14, evaluating loss 2.305370817565918, evaluating accuracy 10.44\n",
      "epoch 15, training loss 2.296789081573486, training accuracy 12.395\n",
      "epoch 15, evaluating loss 2.3051339839935303, evaluating accuracy 10.13\n",
      "epoch 16, training loss 2.2956632522583007, training accuracy 12.59\n",
      "epoch 16, evaluating loss 2.306520178222656, evaluating accuracy 10.24\n",
      "epoch 17, training loss 2.294822999954224, training accuracy 12.7875\n",
      "epoch 17, evaluating loss 2.3066447994232178, evaluating accuracy 9.72\n",
      "epoch 18, training loss 2.293758982849121, training accuracy 13.165\n",
      "epoch 18, evaluating loss 2.3068136192321775, evaluating accuracy 9.71\n",
      "epoch 19, training loss 2.292652816390991, training accuracy 13.2\n",
      "epoch 19, evaluating loss 2.3075559131622314, evaluating accuracy 9.85\n",
      "Done training.\n",
      "Current length: 20\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3041816898345946, training accuracy 10.1625\n",
      "epoch 0, evaluating loss 2.303434575653076, evaluating accuracy 9.84\n",
      "epoch 1, training loss 2.30349260635376, training accuracy 10.0525\n",
      "epoch 1, evaluating loss 2.30386460647583, evaluating accuracy 10.12\n",
      "epoch 2, training loss 2.3031976196289063, training accuracy 10.0675\n",
      "epoch 2, evaluating loss 2.304319421005249, evaluating accuracy 9.59\n",
      "epoch 3, training loss 2.3030443630218507, training accuracy 9.9625\n",
      "epoch 3, evaluating loss 2.303777420043945, evaluating accuracy 10.38\n",
      "epoch 4, training loss 2.302694301223755, training accuracy 10.44\n",
      "epoch 4, evaluating loss 2.3040220497131347, evaluating accuracy 10.25\n",
      "epoch 5, training loss 2.3024556686401367, training accuracy 10.4225\n",
      "epoch 5, evaluating loss 2.3039622356414795, evaluating accuracy 10.04\n",
      "epoch 6, training loss 2.3021457954406737, training accuracy 10.875\n",
      "epoch 6, evaluating loss 2.3043015014648436, evaluating accuracy 9.65\n",
      "epoch 7, training loss 2.3018944812774658, training accuracy 10.74\n",
      "epoch 7, evaluating loss 2.3047038257598875, evaluating accuracy 9.77\n",
      "epoch 8, training loss 2.3016821178436278, training accuracy 10.6725\n",
      "epoch 8, evaluating loss 2.3048207084655763, evaluating accuracy 9.62\n",
      "epoch 9, training loss 2.3012537731170655, training accuracy 10.98\n",
      "epoch 9, evaluating loss 2.304882398223877, evaluating accuracy 10.01\n",
      "epoch 10, training loss 2.3008530292510985, training accuracy 11.1225\n",
      "epoch 10, evaluating loss 2.304703420639038, evaluating accuracy 9.41\n",
      "epoch 11, training loss 2.300614405441284, training accuracy 11.145\n",
      "epoch 11, evaluating loss 2.3063808464050295, evaluating accuracy 9.61\n",
      "epoch 12, training loss 2.300004793167114, training accuracy 11.4825\n",
      "epoch 12, evaluating loss 2.3053046070098877, evaluating accuracy 9.88\n",
      "epoch 13, training loss 2.2994561893463135, training accuracy 11.5075\n",
      "epoch 13, evaluating loss 2.3055603507995603, evaluating accuracy 10.05\n",
      "epoch 14, training loss 2.2988635833740236, training accuracy 11.73\n",
      "epoch 14, evaluating loss 2.3056881507873537, evaluating accuracy 9.75\n",
      "epoch 15, training loss 2.2981899890899657, training accuracy 12.1\n",
      "epoch 15, evaluating loss 2.306763254928589, evaluating accuracy 10.29\n",
      "epoch 16, training loss 2.297472443008423, training accuracy 11.89\n",
      "epoch 16, evaluating loss 2.305998073196411, evaluating accuracy 9.69\n",
      "epoch 17, training loss 2.297014221954346, training accuracy 12.275\n",
      "epoch 17, evaluating loss 2.306391332626343, evaluating accuracy 9.96\n",
      "epoch 18, training loss 2.2960804035186766, training accuracy 12.4625\n",
      "epoch 18, evaluating loss 2.30673137550354, evaluating accuracy 9.84\n",
      "epoch 19, training loss 2.2953254318237306, training accuracy 12.6825\n",
      "epoch 19, evaluating loss 2.307680176925659, evaluating accuracy 9.78\n",
      "Done training.\n",
      "Current length: 21\n",
      "device: cuda\n",
      "epoch 0, training loss 2.304276096725464, training accuracy 9.985\n",
      "epoch 0, evaluating loss 2.3038544517517088, evaluating accuracy 10.46\n",
      "epoch 1, training loss 2.303308359527588, training accuracy 9.7725\n",
      "epoch 1, evaluating loss 2.3039869300842284, evaluating accuracy 10.09\n",
      "epoch 2, training loss 2.3029187534332274, training accuracy 10.3225\n",
      "epoch 2, evaluating loss 2.3032656799316404, evaluating accuracy 9.82\n",
      "epoch 3, training loss 2.3026706653594973, training accuracy 10.42\n",
      "epoch 3, evaluating loss 2.3038261516571046, evaluating accuracy 10.46\n",
      "epoch 4, training loss 2.302300609588623, training accuracy 10.6375\n",
      "epoch 4, evaluating loss 2.303831815338135, evaluating accuracy 10.41\n",
      "epoch 5, training loss 2.3021234897613527, training accuracy 10.48\n",
      "epoch 5, evaluating loss 2.3036279205322265, evaluating accuracy 10.0\n",
      "epoch 6, training loss 2.3016897762298583, training accuracy 10.9025\n",
      "epoch 6, evaluating loss 2.3043844276428223, evaluating accuracy 10.16\n",
      "epoch 7, training loss 2.2316667320251464, training accuracy 18.2025\n",
      "epoch 7, evaluating loss 2.2181518589019777, evaluating accuracy 19.08\n",
      "epoch 8, training loss 2.2163952407836915, training accuracy 19.71\n",
      "epoch 8, evaluating loss 2.2158128784179687, evaluating accuracy 20.77\n",
      "epoch 9, training loss 2.2092160221099855, training accuracy 20.365\n",
      "epoch 9, evaluating loss 2.2075013469696043, evaluating accuracy 19.95\n",
      "epoch 10, training loss 2.208895032501221, training accuracy 20.565\n",
      "epoch 10, evaluating loss 2.207932975769043, evaluating accuracy 20.02\n",
      "epoch 11, training loss 2.2089131748199464, training accuracy 20.29\n",
      "epoch 11, evaluating loss 2.2090356159210205, evaluating accuracy 20.0\n",
      "epoch 12, training loss 2.2085769222259524, training accuracy 20.5975\n",
      "epoch 12, evaluating loss 2.208611679458618, evaluating accuracy 20.13\n",
      "epoch 13, training loss 2.2081767978668214, training accuracy 20.6925\n",
      "epoch 13, evaluating loss 2.2070171882629395, evaluating accuracy 21.25\n",
      "epoch 14, training loss 2.207777788925171, training accuracy 20.755\n",
      "epoch 14, evaluating loss 2.2077748725891113, evaluating accuracy 21.09\n",
      "epoch 15, training loss 2.2079812320709227, training accuracy 20.825\n",
      "epoch 15, evaluating loss 2.210247032546997, evaluating accuracy 20.52\n",
      "epoch 16, training loss 2.214215382003784, training accuracy 20.365\n",
      "epoch 16, evaluating loss 2.2087484088897704, evaluating accuracy 20.19\n",
      "epoch 17, training loss 2.2076309047698977, training accuracy 20.8725\n",
      "epoch 17, evaluating loss 2.209082372665405, evaluating accuracy 20.68\n",
      "epoch 18, training loss 2.207142446899414, training accuracy 21.0425\n",
      "epoch 18, evaluating loss 2.2087565814971923, evaluating accuracy 20.13\n",
      "epoch 19, training loss 2.2153096115112305, training accuracy 20.785\n",
      "epoch 19, evaluating loss 2.226299519729614, evaluating accuracy 19.99\n",
      "Done training.\n",
      "Current length: 22\n",
      "device: cuda\n",
      "epoch 0, training loss 2.304364196777344, training accuracy 9.9225\n",
      "epoch 0, evaluating loss 2.3035559535980226, evaluating accuracy 9.93\n",
      "epoch 1, training loss 2.3030267707824708, training accuracy 10.3175\n",
      "epoch 1, evaluating loss 2.302994110107422, evaluating accuracy 9.99\n",
      "epoch 2, training loss 2.3026216148376464, training accuracy 10.4475\n",
      "epoch 2, evaluating loss 2.3044666324615477, evaluating accuracy 10.22\n",
      "epoch 3, training loss 2.302388437652588, training accuracy 10.515\n",
      "epoch 3, evaluating loss 2.3028602828979494, evaluating accuracy 10.64\n",
      "epoch 4, training loss 2.3020883472442626, training accuracy 10.6825\n",
      "epoch 4, evaluating loss 2.3038114433288572, evaluating accuracy 10.21\n",
      "epoch 5, training loss 2.301728044128418, training accuracy 10.5025\n",
      "epoch 5, evaluating loss 2.3032565589904785, evaluating accuracy 10.46\n",
      "epoch 6, training loss 2.3013405281066897, training accuracy 10.8975\n",
      "epoch 6, evaluating loss 2.3046704372406004, evaluating accuracy 9.88\n",
      "epoch 7, training loss 2.3008728439331056, training accuracy 11.265\n",
      "epoch 7, evaluating loss 2.3055515365600585, evaluating accuracy 10.16\n",
      "epoch 8, training loss 2.3005416511535643, training accuracy 11.105\n",
      "epoch 8, evaluating loss 2.3039174018859865, evaluating accuracy 10.02\n",
      "epoch 9, training loss 2.2998734954833986, training accuracy 11.4125\n",
      "epoch 9, evaluating loss 2.3037744537353517, evaluating accuracy 10.47\n",
      "epoch 10, training loss 2.2994059490203855, training accuracy 11.4725\n",
      "epoch 10, evaluating loss 2.3048416931152342, evaluating accuracy 10.37\n",
      "epoch 11, training loss 2.298683055114746, training accuracy 11.785\n",
      "epoch 11, evaluating loss 2.303974214172363, evaluating accuracy 10.58\n",
      "epoch 12, training loss 2.2979859077453613, training accuracy 12.0425\n",
      "epoch 12, evaluating loss 2.3046263671875, evaluating accuracy 9.84\n",
      "epoch 13, training loss 2.297235065460205, training accuracy 12.285\n",
      "epoch 13, evaluating loss 2.3059921920776367, evaluating accuracy 10.09\n",
      "epoch 14, training loss 2.296287425613403, training accuracy 12.555\n",
      "epoch 14, evaluating loss 2.3066130390167237, evaluating accuracy 10.02\n",
      "epoch 15, training loss 2.2954161491394043, training accuracy 12.725\n",
      "epoch 15, evaluating loss 2.3062427341461182, evaluating accuracy 10.39\n",
      "epoch 16, training loss 2.2944620014190673, training accuracy 12.8625\n",
      "epoch 16, evaluating loss 2.306381052398682, evaluating accuracy 10.23\n",
      "epoch 17, training loss 2.2932487312316896, training accuracy 13.3375\n",
      "epoch 17, evaluating loss 2.307822261047363, evaluating accuracy 9.64\n",
      "epoch 18, training loss 2.291989749145508, training accuracy 13.435\n",
      "epoch 18, evaluating loss 2.307658485794067, evaluating accuracy 10.01\n",
      "epoch 19, training loss 2.2904432609558105, training accuracy 13.85\n",
      "epoch 19, evaluating loss 2.308869030380249, evaluating accuracy 10.08\n",
      "Done training.\n",
      "Current length: 23\n",
      "device: cuda\n",
      "epoch 0, training loss 2.3042441291809084, training accuracy 10.06\n",
      "epoch 0, evaluating loss 2.3030837886810303, evaluating accuracy 10.45\n",
      "epoch 1, training loss 2.3033084743499757, training accuracy 10.19\n",
      "epoch 1, evaluating loss 2.3039664585113524, evaluating accuracy 10.23\n",
      "epoch 2, training loss 2.3030672721862793, training accuracy 10.145\n",
      "epoch 2, evaluating loss 2.303749663925171, evaluating accuracy 9.72\n",
      "epoch 3, training loss 2.3027590522766115, training accuracy 10.575\n",
      "epoch 3, evaluating loss 2.3035948677062987, evaluating accuracy 10.44\n",
      "epoch 4, training loss 2.302573804092407, training accuracy 10.58\n",
      "epoch 4, evaluating loss 2.3077568210601807, evaluating accuracy 9.35\n",
      "epoch 5, training loss 2.3025594024658202, training accuracy 10.6775\n",
      "epoch 5, evaluating loss 2.3045188144683837, evaluating accuracy 9.71\n",
      "epoch 6, training loss 2.3022600971221925, training accuracy 10.6275\n",
      "epoch 6, evaluating loss 2.3039142028808595, evaluating accuracy 10.02\n",
      "epoch 7, training loss 2.301873975753784, training accuracy 10.655\n",
      "epoch 7, evaluating loss 2.3048906326293945, evaluating accuracy 9.41\n",
      "epoch 8, training loss 2.3016120765686034, training accuracy 11.0575\n",
      "epoch 8, evaluating loss 2.30412579498291, evaluating accuracy 9.73\n",
      "epoch 9, training loss 2.3009329734802244, training accuracy 11.045\n",
      "epoch 9, evaluating loss 2.3048023872375487, evaluating accuracy 9.74\n",
      "epoch 10, training loss 2.3007693923950194, training accuracy 11.355\n",
      "epoch 10, evaluating loss 2.3053168884277344, evaluating accuracy 10.18\n",
      "epoch 11, training loss 2.3004927055358886, training accuracy 11.265\n",
      "epoch 11, evaluating loss 2.3049907585144043, evaluating accuracy 9.46\n",
      "epoch 12, training loss 2.299994079208374, training accuracy 11.28\n",
      "epoch 12, evaluating loss 2.305385065841675, evaluating accuracy 9.54\n",
      "epoch 13, training loss 2.2995184757232665, training accuracy 11.5475\n",
      "epoch 13, evaluating loss 2.3065123104095457, evaluating accuracy 9.13\n",
      "epoch 14, training loss 2.2989793605804443, training accuracy 11.6575\n",
      "epoch 14, evaluating loss 2.3066582263946533, evaluating accuracy 9.56\n",
      "epoch 15, training loss 2.298512816619873, training accuracy 11.73\n",
      "epoch 15, evaluating loss 2.3073397830963134, evaluating accuracy 9.54\n",
      "epoch 16, training loss 2.297859141540527, training accuracy 12.0375\n",
      "epoch 16, evaluating loss 2.3066972980499267, evaluating accuracy 9.72\n",
      "epoch 17, training loss 2.297287260437012, training accuracy 12.225\n",
      "epoch 17, evaluating loss 2.3068699531555175, evaluating accuracy 9.68\n",
      "epoch 18, training loss 2.2965172763824464, training accuracy 12.485\n",
      "epoch 18, evaluating loss 2.3082546981811523, evaluating accuracy 9.6\n",
      "epoch 19, training loss 2.2957465412139895, training accuracy 12.3675\n",
      "epoch 19, evaluating loss 2.3074860317230224, evaluating accuracy 9.93\n",
      "Done training.\n",
      "Current length: 24\n",
      "device: cuda\n",
      "epoch 0, training loss 2.304250979232788, training accuracy 9.9675\n",
      "epoch 0, evaluating loss 2.3028857208251954, evaluating accuracy 10.09\n",
      "epoch 1, training loss 2.3035977893829345, training accuracy 9.715\n",
      "epoch 1, evaluating loss 2.302949360656738, evaluating accuracy 10.06\n",
      "epoch 2, training loss 2.303428790283203, training accuracy 10.0275\n",
      "epoch 2, evaluating loss 2.3035181175231934, evaluating accuracy 9.15\n",
      "epoch 3, training loss 2.3031555740356446, training accuracy 10.255\n",
      "epoch 3, evaluating loss 2.3032536308288574, evaluating accuracy 9.92\n",
      "epoch 4, training loss 2.3030004692077637, training accuracy 10.1375\n",
      "epoch 4, evaluating loss 2.3040706756591796, evaluating accuracy 10.21\n",
      "epoch 5, training loss 2.3030126350402833, training accuracy 10.185\n",
      "epoch 5, evaluating loss 2.3037122982025147, evaluating accuracy 10.6\n",
      "epoch 6, training loss 2.302779005432129, training accuracy 10.2975\n",
      "epoch 6, evaluating loss 2.30424469909668, evaluating accuracy 10.09\n",
      "epoch 7, training loss 2.3026644050598146, training accuracy 10.4575\n",
      "epoch 7, evaluating loss 2.3027481018066407, evaluating accuracy 10.23\n",
      "epoch 8, training loss 2.3025772365570067, training accuracy 10.4425\n",
      "epoch 8, evaluating loss 2.3031961589813235, evaluating accuracy 10.02\n",
      "epoch 9, training loss 2.302349249267578, training accuracy 10.5125\n",
      "epoch 9, evaluating loss 2.3035006759643553, evaluating accuracy 10.39\n",
      "epoch 10, training loss 2.302003396987915, training accuracy 10.685\n",
      "epoch 10, evaluating loss 2.303339966964722, evaluating accuracy 10.16\n",
      "epoch 11, training loss 2.30190419960022, training accuracy 10.69\n",
      "epoch 11, evaluating loss 2.304154033279419, evaluating accuracy 10.3\n",
      "epoch 12, training loss 2.30162244720459, training accuracy 10.7025\n",
      "epoch 12, evaluating loss 2.3039029300689697, evaluating accuracy 10.57\n",
      "epoch 13, training loss 2.3011223274230956, training accuracy 10.94\n",
      "epoch 13, evaluating loss 2.304083588027954, evaluating accuracy 9.89\n",
      "epoch 14, training loss 2.3009363777160643, training accuracy 10.8725\n",
      "epoch 14, evaluating loss 2.3050203132629394, evaluating accuracy 10.1\n",
      "epoch 15, training loss 2.3003704551696775, training accuracy 11.2225\n",
      "epoch 15, evaluating loss 2.304744023895264, evaluating accuracy 9.87\n",
      "epoch 16, training loss 2.241932578659058, training accuracy 17.5525\n",
      "epoch 16, evaluating loss 2.22057194480896, evaluating accuracy 19.6\n",
      "epoch 17, training loss 2.229286217498779, training accuracy 18.1825\n",
      "epoch 17, evaluating loss 2.233227407836914, evaluating accuracy 18.04\n",
      "epoch 18, training loss 2.231488088989258, training accuracy 18.0025\n",
      "epoch 18, evaluating loss 2.2318097095489504, evaluating accuracy 17.61\n",
      "epoch 19, training loss 2.2217283794403078, training accuracy 19.1975\n",
      "epoch 19, evaluating loss 2.212717694091797, evaluating accuracy 19.83\n",
      "Done training.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "from train import main\n",
    "\n",
    "config = Namespace(\n",
    "    input_length=2,\n",
    "    input_dim=1,\n",
    "    num_classes=10,\n",
    "    num_hidden=128,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    max_epoch=20,\n",
    "    max_norm=10.0,\n",
    "    data_size=50000,\n",
    "    portion_train=0.8\n",
    ")\n",
    "\n",
    "x = list(range(3, 25))\n",
    "y = []\n",
    "\n",
    "for T in x:\n",
    "    print(f'Current length: {T}')\n",
    "    config.input_length = T\n",
    "    y.append(main(config)[1])  # accuracy\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y, marker='o', color='b', label='Accuracy')\n",
    "plt.xlabel('Length', fontsize=12)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-16T02:27:34.938269100Z",
     "start_time": "2024-11-16T02:05:30.189619100Z"
    }
   },
   "id": "1dd986138cc5f86b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
